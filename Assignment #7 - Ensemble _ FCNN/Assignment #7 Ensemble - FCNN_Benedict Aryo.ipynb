{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisi Masalah\n",
    "\n",
    "Tenggelamnya kapal RMS Titanic adalah salah satu tragedi yang paling terkenal. Pada tanggal 15 April 1912, dalam pelayaran perdana ny, kapal Titanic tenggelam setelah bertabrakan dengan sebuah gunung es, mengakibatkan 1502 korban jiwa dari 2224 penumpan dan awak kapal. Tragedi ini sangat mengguncang komunitas internasional dan mendorong untuk membuat peraturan keselamatan yang lebih baik di dalam kapal dan pelayaran.\n",
    "\n",
    "Salah satu alasan kapal Titanic mengakibatkan korban jiwa yang relatif banyak diakibatkan karena tidak cukupnya jumlah kapal penyelemat (skoci) bagi para penumpang dan awak kapal. Meskipun ada yang beruntung selamat pada saat proses penenggelaman, kelompok orang yang lebih banyak selamat dibandingkan yang lain antara lain para wanita, anak - anak, dan para kelas atas.\n",
    "\n",
    "Dalam tugas ini, kita meminta anda untuk menyelesaikan analisis mengenai kriteria seperti apa orang yang selamat dari tragedi kapal Titanic. Khususnya, kita meminta anda untuk menerapkan <i>machine learning</i> untuk memprediksi penumpang mana saja yang selamat dari tragedi tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "1. Tim pengajar telah menyiapkan data yang sudah dieksplorasi dan dibersihkan sehingga siap untuk dimasukkan ke dalam model\n",
    "2. Tim pengajar telah menyiapkan 2 model yang sudah di-<i>training</i> dengan menggunakan algoritma <i>Random Forest</i> dan <i>Adaboost</i>\n",
    "3. Buatlah model FCNN untuk memperkuat analisis dan prediksi anda\n",
    "5. Buatlah model <i>Ensemble</i> yang menggabungkan model <i>Random Forest</i>, <i>Adaboost</i>, dan <i>FCNN</i>. Anda dapat memilih untuk menggunakan <i>Majority Vote</i> atau <i>Stacking</i>\n",
    "6. Buatlah tabel perbandingan mengenai performa keempat model yang telah dibuat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benedict\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/titanic.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah deskripsi dari masing - masing kolom:\n",
    "- survival = Selamat atau tidaknya penumpang yang bersangkutan (0 = Tidak, 1 = Selamat)\n",
    "- pclass = Kelas tiket penumpang (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- sex = Gender\n",
    "- Age = Umur dalam tahun\n",
    "- sibsp = Jumlah saudara kandung/pasangan yang bersama dengan penumpang bersangkutan di atas kapal\n",
    "- parch = Jumlah orang tua/anak - anak yang bersama dengan penumpang bersangkutan di atsa kapal\n",
    "- fare = Jumlah biaya perjalanan penumpang\n",
    "- cabin = Nomor kabin\n",
    "- embarked = Nama pelabuhan tempat penumpang berangkat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Embarked     Fare  Parch  Pclass     Sex  SibSp  Survived\n",
       "0   22        S   7.2500      0       3    male      1       0.0\n",
       "1   38        C  71.2833      0       1  female      1       1.0\n",
       "2   26        S   7.9250      0       3  female      0       1.0\n",
       "3   35        S  53.1000      0       1  female      1       1.0\n",
       "4   35        S   8.0500      0       3    male      0       0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = df.loc[:, [\"Age\", \"Fare\", \"Parch\", \"SibSp\"]].copy()\n",
    "categorical_data = df.loc[:, [\"Embarked\", \"Pclass\", \"Sex\",]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Scaling numerical data\n",
    "sc = StandardScaler()\n",
    "numeric_data = sc.fit_transform(numeric_data)\n",
    "numeric_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>pclass_1</th>\n",
       "      <th>pclass_2</th>\n",
       "      <th>pclass_3</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   embarked_C  embarked_Q  embarked_S  pclass_1  pclass_2  pclass_3  \\\n",
       "0           0           0           1         0         0         1   \n",
       "1           1           0           0         1         0         0   \n",
       "2           0           0           1         0         0         1   \n",
       "3           0           0           1         1         0         0   \n",
       "4           0           0           1         0         0         1   \n",
       "\n",
       "   sex_female  sex_male  \n",
       "0           0         1  \n",
       "1           1         0  \n",
       "2           1         0  \n",
       "3           1         0  \n",
       "4           0         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Binarizer categorical data\n",
    "embarked_dummy = pd.get_dummies(categorical_data[\"Embarked\"], prefix=\"embarked\")\n",
    "categorical_data = categorical_data.drop([\"Embarked\"], axis=1)\n",
    "categorical_data = pd.concat([categorical_data, embarked_dummy], axis=1)\n",
    "\n",
    "pclass_dummy = pd.get_dummies(categorical_data[\"Pclass\"], prefix=\"pclass\")\n",
    "categorical_data = categorical_data.drop([\"Pclass\"], axis=1)\n",
    "categorical_data = pd.concat([categorical_data, pclass_dummy], axis=1)\n",
    "\n",
    "sex_dummy = pd.get_dummies(categorical_data[\"Sex\"], prefix=\"sex\")\n",
    "categorical_data = categorical_data.drop([\"Sex\"], axis=1)\n",
    "categorical_data = pd.concat([categorical_data, sex_dummy], axis=1)\n",
    "\n",
    "categorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>pclass_1</th>\n",
       "      <th>pclass_2</th>\n",
       "      <th>pclass_3</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.510769</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.579769</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.238134</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375293</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375293</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age      Fare     Parch     SibSp  Survived  embarked_C  embarked_Q  \\\n",
       "0 -0.510769 -0.502445 -0.473674  0.432793       0.0           0           0   \n",
       "1  0.579769  0.786845 -0.473674  0.432793       1.0           1           0   \n",
       "2 -0.238134 -0.488854 -0.473674 -0.474545       1.0           0           0   \n",
       "3  0.375293  0.420730 -0.473674  0.432793       1.0           0           0   \n",
       "4  0.375293 -0.486337 -0.473674 -0.474545       0.0           0           0   \n",
       "\n",
       "   embarked_S  pclass_1  pclass_2  pclass_3  sex_female  sex_male  \n",
       "0           1         0         0         1           0         1  \n",
       "1           0         1         0         0           1         0  \n",
       "2           1         0         0         1           1         0  \n",
       "3           1         1         0         0           1         0  \n",
       "4           1         0         0         1           0         1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the preprocessed numerical data & categorical data\n",
    "df = df.loc[:, [\"Age\", \"Fare\", \"Parch\", \"SibSp\", \"Survived\"]].copy()\n",
    "df.loc[:, \"Age\":\"SibSp\"] = numeric_data\n",
    "df = pd.concat([df, categorical_data], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = df.drop([\"Survived\"], axis=1)\n",
    "y = df[\"Survived\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1 Model - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benedict\\miniconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.0 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\benedict\\miniconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.0 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=123, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"./model/rf.sav\"\n",
    "rf_model = pickle.load(open(filename, 'rb'))\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Model - AdaBoost Classifier\n",
    "\n",
    "One of many boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benedict\\miniconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.0 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "c:\\users\\benedict\\miniconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 0.19.0 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.1, n_estimators=1000, random_state=123)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"./model/ad.sav\"\n",
    "ab_model = pickle.load(open(filename, 'rb'))\n",
    "ab_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Model - FCNN (deep learning) (50 point)\n",
    "\n",
    "Buatlah model FCNN dengan menggunakan Keras yang dapat memprediksi target (survival) dari kasus di atas. Anda dapat membuat <i>neural network</i> dengan menggunakan 2 <i>hidden layer</i>. \n",
    "\n",
    "NB: Jangan lupa untuk meng-<i>import</i> <i>Keras library</i> terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(801, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Input',\n",
       " 'Model',\n",
       " 'Sequential',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'activations',\n",
       " 'applications',\n",
       " 'backend',\n",
       " 'callbacks',\n",
       " 'constraints',\n",
       " 'datasets',\n",
       " 'engine',\n",
       " 'initializers',\n",
       " 'layers',\n",
       " 'legacy',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'models',\n",
       " 'optimizers',\n",
       " 'preprocessing',\n",
       " 'regularizers',\n",
       " 'utils',\n",
       " 'wrappers']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "dir(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "801/801 [==============================] - 0s 360us/step - loss: 0.6896 - acc: 0.6055 - val_loss: 0.6806 - val_acc: 0.6889\n",
      "Epoch 2/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.6781 - acc: 0.6142 - val_loss: 0.6567 - val_acc: 0.7333\n",
      "Epoch 3/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.6526 - acc: 0.6604 - val_loss: 0.6057 - val_acc: 0.8000\n",
      "Epoch 4/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.6086 - acc: 0.7054 - val_loss: 0.5310 - val_acc: 0.8222\n",
      "Epoch 5/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.5552 - acc: 0.7303 - val_loss: 0.4599 - val_acc: 0.8444\n",
      "Epoch 6/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.5124 - acc: 0.7740 - val_loss: 0.4197 - val_acc: 0.8667\n",
      "Epoch 7/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.4788 - acc: 0.7978 - val_loss: 0.3853 - val_acc: 0.8444\n",
      "Epoch 8/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.4632 - acc: 0.7965 - val_loss: 0.3700 - val_acc: 0.8444\n",
      "Epoch 9/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.4531 - acc: 0.8027 - val_loss: 0.3652 - val_acc: 0.8444\n",
      "Epoch 10/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.4481 - acc: 0.8052 - val_loss: 0.3599 - val_acc: 0.8444\n",
      "Epoch 11/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.4420 - acc: 0.8065 - val_loss: 0.3632 - val_acc: 0.8444\n",
      "Epoch 12/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.4364 - acc: 0.8115 - val_loss: 0.3654 - val_acc: 0.8444\n",
      "Epoch 13/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.4348 - acc: 0.8140 - val_loss: 0.3628 - val_acc: 0.8444\n",
      "Epoch 14/200\n",
      "801/801 [==============================] - 0s 15us/step - loss: 0.4286 - acc: 0.8140 - val_loss: 0.3628 - val_acc: 0.8444\n",
      "Epoch 15/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.4275 - acc: 0.8127 - val_loss: 0.3516 - val_acc: 0.8667\n",
      "Epoch 16/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.4229 - acc: 0.8177 - val_loss: 0.3675 - val_acc: 0.8444\n",
      "Epoch 17/200\n",
      "801/801 [==============================] - 0s 15us/step - loss: 0.4201 - acc: 0.8202 - val_loss: 0.3583 - val_acc: 0.8556\n",
      "Epoch 18/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.4180 - acc: 0.8240 - val_loss: 0.3557 - val_acc: 0.8556\n",
      "Epoch 19/200\n",
      "801/801 [==============================] - 0s 15us/step - loss: 0.4158 - acc: 0.8252 - val_loss: 0.3499 - val_acc: 0.8667\n",
      "Epoch 20/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.4136 - acc: 0.8290 - val_loss: 0.3623 - val_acc: 0.8444\n",
      "Epoch 21/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.4105 - acc: 0.8165 - val_loss: 0.3680 - val_acc: 0.8444\n",
      "Epoch 22/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.4087 - acc: 0.8227 - val_loss: 0.3529 - val_acc: 0.8556\n",
      "Epoch 23/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.4068 - acc: 0.8315 - val_loss: 0.3581 - val_acc: 0.8556\n",
      "Epoch 24/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.4058 - acc: 0.8265 - val_loss: 0.3597 - val_acc: 0.8667\n",
      "Epoch 25/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.4039 - acc: 0.8290 - val_loss: 0.3579 - val_acc: 0.8667\n",
      "Epoch 26/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.4033 - acc: 0.8315 - val_loss: 0.3577 - val_acc: 0.8556\n",
      "Epoch 27/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.4020 - acc: 0.8315 - val_loss: 0.3595 - val_acc: 0.8556\n",
      "Epoch 28/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.4010 - acc: 0.8290 - val_loss: 0.3554 - val_acc: 0.8667\n",
      "Epoch 29/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.3990 - acc: 0.8365 - val_loss: 0.3567 - val_acc: 0.8667\n",
      "Epoch 30/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.3989 - acc: 0.8327 - val_loss: 0.3569 - val_acc: 0.8778\n",
      "Epoch 31/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.3996 - acc: 0.8315 - val_loss: 0.3552 - val_acc: 0.8889\n",
      "Epoch 32/200\n",
      "801/801 [==============================] - 0s 16us/step - loss: 0.3969 - acc: 0.8302 - val_loss: 0.3511 - val_acc: 0.8889\n",
      "Epoch 33/200\n",
      "801/801 [==============================] - 0s 17us/step - loss: 0.3947 - acc: 0.8340 - val_loss: 0.3597 - val_acc: 0.8778\n",
      "Epoch 34/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3974 - acc: 0.8365 - val_loss: 0.3574 - val_acc: 0.8889\n",
      "Epoch 35/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3975 - acc: 0.8377 - val_loss: 0.3551 - val_acc: 0.8778\n",
      "Epoch 36/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3946 - acc: 0.8390 - val_loss: 0.3671 - val_acc: 0.8556\n",
      "Epoch 37/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3928 - acc: 0.8427 - val_loss: 0.3531 - val_acc: 0.8889\n",
      "Epoch 38/200\n",
      "801/801 [==============================] - 0s 34us/step - loss: 0.3949 - acc: 0.8377 - val_loss: 0.3584 - val_acc: 0.8889\n",
      "Epoch 39/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3928 - acc: 0.8340 - val_loss: 0.3562 - val_acc: 0.8778\n",
      "Epoch 40/200\n",
      "801/801 [==============================] - ETA: 0s - loss: 0.4218 - acc: 0.812 - 0s 25us/step - loss: 0.3925 - acc: 0.8327 - val_loss: 0.3691 - val_acc: 0.8667\n",
      "Epoch 41/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3911 - acc: 0.8352 - val_loss: 0.3463 - val_acc: 0.8889\n",
      "Epoch 42/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3898 - acc: 0.8402 - val_loss: 0.3686 - val_acc: 0.8667\n",
      "Epoch 43/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3891 - acc: 0.8302 - val_loss: 0.3515 - val_acc: 0.8889\n",
      "Epoch 44/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3879 - acc: 0.8414 - val_loss: 0.3619 - val_acc: 0.8667\n",
      "Epoch 45/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3885 - acc: 0.8402 - val_loss: 0.3621 - val_acc: 0.8778\n",
      "Epoch 46/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3861 - acc: 0.8402 - val_loss: 0.3593 - val_acc: 0.8778\n",
      "Epoch 47/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3879 - acc: 0.8365 - val_loss: 0.3625 - val_acc: 0.8778\n",
      "Epoch 48/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3853 - acc: 0.8365 - val_loss: 0.3519 - val_acc: 0.8889\n",
      "Epoch 49/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3855 - acc: 0.8390 - val_loss: 0.3555 - val_acc: 0.8889\n",
      "Epoch 50/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3864 - acc: 0.8377 - val_loss: 0.3530 - val_acc: 0.8889\n",
      "Epoch 51/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3851 - acc: 0.8377 - val_loss: 0.3546 - val_acc: 0.8889\n",
      "Epoch 52/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3836 - acc: 0.8390 - val_loss: 0.3665 - val_acc: 0.8667\n",
      "Epoch 53/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3846 - acc: 0.8402 - val_loss: 0.3594 - val_acc: 0.8778\n",
      "Epoch 54/200\n",
      "801/801 [==============================] - ETA: 0s - loss: 0.3649 - acc: 0.875 - 0s 25us/step - loss: 0.3835 - acc: 0.8402 - val_loss: 0.3527 - val_acc: 0.8889\n",
      "Epoch 55/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3819 - acc: 0.8390 - val_loss: 0.3628 - val_acc: 0.8667\n",
      "Epoch 56/200\n",
      "801/801 [==============================] - 0s 35us/step - loss: 0.3829 - acc: 0.8439 - val_loss: 0.3573 - val_acc: 0.8667\n",
      "Epoch 57/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3814 - acc: 0.8402 - val_loss: 0.3567 - val_acc: 0.8778\n",
      "Epoch 58/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3812 - acc: 0.8402 - val_loss: 0.3588 - val_acc: 0.8889\n",
      "Epoch 59/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3809 - acc: 0.8427 - val_loss: 0.3551 - val_acc: 0.8889\n",
      "Epoch 60/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3813 - acc: 0.8414 - val_loss: 0.3598 - val_acc: 0.8778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "801/801 [==============================] - 0s 36us/step - loss: 0.3803 - acc: 0.8477 - val_loss: 0.3626 - val_acc: 0.8778\n",
      "Epoch 62/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3789 - acc: 0.8439 - val_loss: 0.3558 - val_acc: 0.8778\n",
      "Epoch 63/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3787 - acc: 0.8414 - val_loss: 0.3680 - val_acc: 0.8667\n",
      "Epoch 64/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3782 - acc: 0.8427 - val_loss: 0.3558 - val_acc: 0.8889\n",
      "Epoch 65/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3780 - acc: 0.8427 - val_loss: 0.3555 - val_acc: 0.8889\n",
      "Epoch 66/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3782 - acc: 0.8439 - val_loss: 0.3662 - val_acc: 0.8667\n",
      "Epoch 67/200\n",
      "801/801 [==============================] - 0s 35us/step - loss: 0.3767 - acc: 0.8402 - val_loss: 0.3561 - val_acc: 0.8889\n",
      "Epoch 68/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3770 - acc: 0.8414 - val_loss: 0.3543 - val_acc: 0.8889\n",
      "Epoch 69/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3764 - acc: 0.8427 - val_loss: 0.3655 - val_acc: 0.8778\n",
      "Epoch 70/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3755 - acc: 0.8439 - val_loss: 0.3584 - val_acc: 0.8667\n",
      "Epoch 71/200\n",
      "801/801 [==============================] - 0s 39us/step - loss: 0.3747 - acc: 0.8452 - val_loss: 0.3633 - val_acc: 0.8667\n",
      "Epoch 72/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3772 - acc: 0.8414 - val_loss: 0.3517 - val_acc: 0.8778\n",
      "Epoch 73/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3731 - acc: 0.8464 - val_loss: 0.3666 - val_acc: 0.8667\n",
      "Epoch 74/200\n",
      "801/801 [==============================] - 0s 34us/step - loss: 0.3757 - acc: 0.8489 - val_loss: 0.3532 - val_acc: 0.8889\n",
      "Epoch 75/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3756 - acc: 0.8452 - val_loss: 0.3621 - val_acc: 0.8667\n",
      "Epoch 76/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3736 - acc: 0.8414 - val_loss: 0.3513 - val_acc: 0.8889\n",
      "Epoch 77/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3724 - acc: 0.8439 - val_loss: 0.3556 - val_acc: 0.8667\n",
      "Epoch 78/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3720 - acc: 0.8452 - val_loss: 0.3583 - val_acc: 0.8667\n",
      "Epoch 79/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3712 - acc: 0.8452 - val_loss: 0.3557 - val_acc: 0.8667\n",
      "Epoch 80/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3720 - acc: 0.8402 - val_loss: 0.3547 - val_acc: 0.8667\n",
      "Epoch 81/200\n",
      "801/801 [==============================] - 0s 34us/step - loss: 0.3707 - acc: 0.8489 - val_loss: 0.3638 - val_acc: 0.8556\n",
      "Epoch 82/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3708 - acc: 0.8439 - val_loss: 0.3574 - val_acc: 0.8556\n",
      "Epoch 83/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3697 - acc: 0.8477 - val_loss: 0.3669 - val_acc: 0.8444\n",
      "Epoch 84/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3682 - acc: 0.8489 - val_loss: 0.3566 - val_acc: 0.8667\n",
      "Epoch 85/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3688 - acc: 0.8514 - val_loss: 0.3615 - val_acc: 0.8444\n",
      "Epoch 86/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3698 - acc: 0.8502 - val_loss: 0.3519 - val_acc: 0.8778\n",
      "Epoch 87/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3685 - acc: 0.8514 - val_loss: 0.3566 - val_acc: 0.8667\n",
      "Epoch 88/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3689 - acc: 0.8439 - val_loss: 0.3578 - val_acc: 0.8556\n",
      "Epoch 89/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3672 - acc: 0.8489 - val_loss: 0.3569 - val_acc: 0.8667\n",
      "Epoch 90/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3669 - acc: 0.8464 - val_loss: 0.3646 - val_acc: 0.8444\n",
      "Epoch 91/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3668 - acc: 0.8477 - val_loss: 0.3595 - val_acc: 0.8667\n",
      "Epoch 92/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3668 - acc: 0.8464 - val_loss: 0.3625 - val_acc: 0.8444\n",
      "Epoch 93/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3660 - acc: 0.8527 - val_loss: 0.3581 - val_acc: 0.8667\n",
      "Epoch 94/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3651 - acc: 0.8514 - val_loss: 0.3577 - val_acc: 0.8444\n",
      "Epoch 95/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3693 - acc: 0.8502 - val_loss: 0.3519 - val_acc: 0.8667\n",
      "Epoch 96/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3663 - acc: 0.8489 - val_loss: 0.3550 - val_acc: 0.8667\n",
      "Epoch 97/200\n",
      "801/801 [==============================] - 0s 34us/step - loss: 0.3644 - acc: 0.8489 - val_loss: 0.3596 - val_acc: 0.8667\n",
      "Epoch 98/200\n",
      "801/801 [==============================] - 0s 30us/step - loss: 0.3648 - acc: 0.8489 - val_loss: 0.3607 - val_acc: 0.8556\n",
      "Epoch 99/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3655 - acc: 0.8489 - val_loss: 0.3617 - val_acc: 0.8667\n",
      "Epoch 100/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3630 - acc: 0.8477 - val_loss: 0.3617 - val_acc: 0.8556\n",
      "Epoch 101/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3641 - acc: 0.8477 - val_loss: 0.3641 - val_acc: 0.8444\n",
      "Epoch 102/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3632 - acc: 0.8489 - val_loss: 0.3662 - val_acc: 0.8444\n",
      "Epoch 103/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3614 - acc: 0.8452 - val_loss: 0.3542 - val_acc: 0.8778\n",
      "Epoch 104/200\n",
      "801/801 [==============================] - 0s 32us/step - loss: 0.3633 - acc: 0.8489 - val_loss: 0.3679 - val_acc: 0.8444\n",
      "Epoch 105/200\n",
      "801/801 [==============================] - 0s 31us/step - loss: 0.3661 - acc: 0.8464 - val_loss: 0.3583 - val_acc: 0.8667\n",
      "Epoch 106/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3642 - acc: 0.8464 - val_loss: 0.3522 - val_acc: 0.8778\n",
      "Epoch 107/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3600 - acc: 0.8527 - val_loss: 0.3727 - val_acc: 0.8444\n",
      "Epoch 108/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3604 - acc: 0.8539 - val_loss: 0.3532 - val_acc: 0.8667\n",
      "Epoch 109/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3611 - acc: 0.8439 - val_loss: 0.3772 - val_acc: 0.8444\n",
      "Epoch 110/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3595 - acc: 0.8564 - val_loss: 0.3506 - val_acc: 0.8889\n",
      "Epoch 111/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3604 - acc: 0.8452 - val_loss: 0.3559 - val_acc: 0.8667\n",
      "Epoch 112/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3624 - acc: 0.8514 - val_loss: 0.3638 - val_acc: 0.8556\n",
      "Epoch 113/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3579 - acc: 0.8502 - val_loss: 0.3475 - val_acc: 0.9000\n",
      "Epoch 114/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3575 - acc: 0.8477 - val_loss: 0.3713 - val_acc: 0.8444\n",
      "Epoch 115/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3596 - acc: 0.8514 - val_loss: 0.3619 - val_acc: 0.8444\n",
      "Epoch 116/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3581 - acc: 0.8402 - val_loss: 0.3664 - val_acc: 0.8556\n",
      "Epoch 117/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3583 - acc: 0.8552 - val_loss: 0.3636 - val_acc: 0.8556\n",
      "Epoch 118/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3583 - acc: 0.8414 - val_loss: 0.3540 - val_acc: 0.8778\n",
      "Epoch 119/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3571 - acc: 0.8439 - val_loss: 0.3666 - val_acc: 0.8556\n",
      "Epoch 120/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3581 - acc: 0.8539 - val_loss: 0.3564 - val_acc: 0.8667\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801/801 [==============================] - 0s 26us/step - loss: 0.3573 - acc: 0.8514 - val_loss: 0.3667 - val_acc: 0.8444\n",
      "Epoch 122/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3572 - acc: 0.8439 - val_loss: 0.3632 - val_acc: 0.8444\n",
      "Epoch 123/200\n",
      "801/801 [==============================] - 0s 29us/step - loss: 0.3565 - acc: 0.8527 - val_loss: 0.3566 - val_acc: 0.8778\n",
      "Epoch 124/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3552 - acc: 0.8514 - val_loss: 0.3642 - val_acc: 0.8556\n",
      "Epoch 125/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3555 - acc: 0.8502 - val_loss: 0.3654 - val_acc: 0.8667\n",
      "Epoch 126/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3549 - acc: 0.8464 - val_loss: 0.3684 - val_acc: 0.8333\n",
      "Epoch 127/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3554 - acc: 0.8577 - val_loss: 0.3629 - val_acc: 0.8444\n",
      "Epoch 128/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3548 - acc: 0.8539 - val_loss: 0.3708 - val_acc: 0.8333\n",
      "Epoch 129/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3538 - acc: 0.8527 - val_loss: 0.3575 - val_acc: 0.8556\n",
      "Epoch 130/200\n",
      "801/801 [==============================] - ETA: 0s - loss: 0.3711 - acc: 0.843 - 0s 25us/step - loss: 0.3536 - acc: 0.8539 - val_loss: 0.3624 - val_acc: 0.8444\n",
      "Epoch 131/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3540 - acc: 0.8552 - val_loss: 0.3630 - val_acc: 0.8556\n",
      "Epoch 132/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3528 - acc: 0.8552 - val_loss: 0.3583 - val_acc: 0.8667\n",
      "Epoch 133/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3557 - acc: 0.8514 - val_loss: 0.3732 - val_acc: 0.8333\n",
      "Epoch 134/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3561 - acc: 0.8577 - val_loss: 0.3542 - val_acc: 0.8667\n",
      "Epoch 135/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3565 - acc: 0.8452 - val_loss: 0.3668 - val_acc: 0.8444\n",
      "Epoch 136/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3518 - acc: 0.8602 - val_loss: 0.3742 - val_acc: 0.8444\n",
      "Epoch 137/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3521 - acc: 0.8502 - val_loss: 0.3597 - val_acc: 0.8889\n",
      "Epoch 138/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3519 - acc: 0.8527 - val_loss: 0.3659 - val_acc: 0.8667\n",
      "Epoch 139/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3496 - acc: 0.8564 - val_loss: 0.3686 - val_acc: 0.8556\n",
      "Epoch 140/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3501 - acc: 0.8527 - val_loss: 0.3618 - val_acc: 0.8556\n",
      "Epoch 141/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3524 - acc: 0.8402 - val_loss: 0.3739 - val_acc: 0.8444\n",
      "Epoch 142/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.3502 - acc: 0.8552 - val_loss: 0.3711 - val_acc: 0.8444\n",
      "Epoch 143/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3505 - acc: 0.8564 - val_loss: 0.3601 - val_acc: 0.8778\n",
      "Epoch 144/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3491 - acc: 0.8514 - val_loss: 0.3733 - val_acc: 0.8556\n",
      "Epoch 145/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3505 - acc: 0.8464 - val_loss: 0.3669 - val_acc: 0.8556\n",
      "Epoch 146/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3489 - acc: 0.8477 - val_loss: 0.3672 - val_acc: 0.8444\n",
      "Epoch 147/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3492 - acc: 0.8527 - val_loss: 0.3933 - val_acc: 0.8444\n",
      "Epoch 148/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3470 - acc: 0.8602 - val_loss: 0.3644 - val_acc: 0.8889\n",
      "Epoch 149/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3492 - acc: 0.8614 - val_loss: 0.3576 - val_acc: 0.8667\n",
      "Epoch 150/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3496 - acc: 0.8552 - val_loss: 0.3798 - val_acc: 0.8333\n",
      "Epoch 151/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3496 - acc: 0.8577 - val_loss: 0.3596 - val_acc: 0.8667\n",
      "Epoch 152/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3480 - acc: 0.8477 - val_loss: 0.3807 - val_acc: 0.8333\n",
      "Epoch 153/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3483 - acc: 0.8514 - val_loss: 0.3652 - val_acc: 0.8444\n",
      "Epoch 154/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3492 - acc: 0.8577 - val_loss: 0.3664 - val_acc: 0.8667\n",
      "Epoch 155/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3464 - acc: 0.8502 - val_loss: 0.3829 - val_acc: 0.8333\n",
      "Epoch 156/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3456 - acc: 0.8577 - val_loss: 0.3676 - val_acc: 0.8667\n",
      "Epoch 157/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3461 - acc: 0.8527 - val_loss: 0.3680 - val_acc: 0.8667\n",
      "Epoch 158/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3457 - acc: 0.8589 - val_loss: 0.3722 - val_acc: 0.8444\n",
      "Epoch 159/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3449 - acc: 0.8614 - val_loss: 0.3683 - val_acc: 0.8444\n",
      "Epoch 160/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3486 - acc: 0.8477 - val_loss: 0.3777 - val_acc: 0.8667\n",
      "Epoch 161/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3490 - acc: 0.8589 - val_loss: 0.3705 - val_acc: 0.8444\n",
      "Epoch 162/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3449 - acc: 0.8639 - val_loss: 0.3766 - val_acc: 0.8333\n",
      "Epoch 163/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3425 - acc: 0.8564 - val_loss: 0.3656 - val_acc: 0.8778\n",
      "Epoch 164/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3414 - acc: 0.8577 - val_loss: 0.3735 - val_acc: 0.8556\n",
      "Epoch 165/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.3437 - acc: 0.8564 - val_loss: 0.3641 - val_acc: 0.8667\n",
      "Epoch 166/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3423 - acc: 0.8552 - val_loss: 0.3788 - val_acc: 0.8667\n",
      "Epoch 167/200\n",
      "801/801 [==============================] - 0s 20us/step - loss: 0.3428 - acc: 0.8652 - val_loss: 0.3645 - val_acc: 0.8778\n",
      "Epoch 168/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3439 - acc: 0.8552 - val_loss: 0.3744 - val_acc: 0.8444\n",
      "Epoch 169/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3423 - acc: 0.8514 - val_loss: 0.3780 - val_acc: 0.8667\n",
      "Epoch 170/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3411 - acc: 0.8652 - val_loss: 0.3797 - val_acc: 0.8222\n",
      "Epoch 171/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3412 - acc: 0.8577 - val_loss: 0.3727 - val_acc: 0.8556\n",
      "Epoch 172/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3427 - acc: 0.8552 - val_loss: 0.3846 - val_acc: 0.8667\n",
      "Epoch 173/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3420 - acc: 0.8639 - val_loss: 0.3644 - val_acc: 0.8667\n",
      "Epoch 174/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3442 - acc: 0.8477 - val_loss: 0.3784 - val_acc: 0.8667\n",
      "Epoch 175/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3443 - acc: 0.8639 - val_loss: 0.3701 - val_acc: 0.8667\n",
      "Epoch 176/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3450 - acc: 0.8552 - val_loss: 0.3878 - val_acc: 0.8667\n",
      "Epoch 177/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3409 - acc: 0.8577 - val_loss: 0.3748 - val_acc: 0.8444\n",
      "Epoch 178/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3386 - acc: 0.8527 - val_loss: 0.3772 - val_acc: 0.8778\n",
      "Epoch 179/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3392 - acc: 0.8602 - val_loss: 0.3769 - val_acc: 0.8444\n",
      "Epoch 180/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3382 - acc: 0.8652 - val_loss: 0.3773 - val_acc: 0.8889\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801/801 [==============================] - 0s 27us/step - loss: 0.3389 - acc: 0.8652 - val_loss: 0.3659 - val_acc: 0.8667\n",
      "Epoch 182/200\n",
      "801/801 [==============================] - 0s 27us/step - loss: 0.3389 - acc: 0.8564 - val_loss: 0.3881 - val_acc: 0.8667\n",
      "Epoch 183/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3388 - acc: 0.8564 - val_loss: 0.3641 - val_acc: 0.8778\n",
      "Epoch 184/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3425 - acc: 0.8502 - val_loss: 0.3919 - val_acc: 0.8444\n",
      "Epoch 185/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3390 - acc: 0.8652 - val_loss: 0.3689 - val_acc: 0.8889\n",
      "Epoch 186/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3358 - acc: 0.8627 - val_loss: 0.3794 - val_acc: 0.8778\n",
      "Epoch 187/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3361 - acc: 0.8589 - val_loss: 0.3767 - val_acc: 0.8444\n",
      "Epoch 188/200\n",
      "801/801 [==============================] - 0s 21us/step - loss: 0.3363 - acc: 0.8552 - val_loss: 0.3843 - val_acc: 0.8667\n",
      "Epoch 189/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3360 - acc: 0.8639 - val_loss: 0.3718 - val_acc: 0.8667\n",
      "Epoch 190/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3368 - acc: 0.8639 - val_loss: 0.3828 - val_acc: 0.8556\n",
      "Epoch 191/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3343 - acc: 0.8652 - val_loss: 0.3805 - val_acc: 0.8778\n",
      "Epoch 192/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3350 - acc: 0.8589 - val_loss: 0.3775 - val_acc: 0.8778\n",
      "Epoch 193/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3346 - acc: 0.8614 - val_loss: 0.3903 - val_acc: 0.8667\n",
      "Epoch 194/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3339 - acc: 0.8614 - val_loss: 0.3753 - val_acc: 0.8778\n",
      "Epoch 195/200\n",
      "801/801 [==============================] - 0s 22us/step - loss: 0.3338 - acc: 0.8627 - val_loss: 0.3841 - val_acc: 0.8667\n",
      "Epoch 196/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3361 - acc: 0.8639 - val_loss: 0.3807 - val_acc: 0.8667\n",
      "Epoch 197/200\n",
      "801/801 [==============================] - 0s 26us/step - loss: 0.3372 - acc: 0.8564 - val_loss: 0.3670 - val_acc: 0.8778\n",
      "Epoch 198/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3356 - acc: 0.8527 - val_loss: 0.3859 - val_acc: 0.8667\n",
      "Epoch 199/200\n",
      "801/801 [==============================] - 0s 24us/step - loss: 0.3325 - acc: 0.8627 - val_loss: 0.3878 - val_acc: 0.8778\n",
      "Epoch 200/200\n",
      "801/801 [==============================] - 0s 25us/step - loss: 0.3322 - acc: 0.8614 - val_loss: 0.3794 - val_acc: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23346eb8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Import Keras\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"Bikin Baseline model\"\n",
    "\n",
    "def keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=12, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))\n",
    "    # Compile Model\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "fcnn_model = KerasClassifier(build_fn=keras_model, epochs=200, batch_size=64, verbose=1, validation_data=(X_test,y_test));\n",
    "fcnn_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#fcnn_model.fit(X_train,y_train, batch_size=64, epochs=200, verbose=1,validation_data=(X_test,y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4 Model - Ensemble (40 point)\n",
    "\n",
    "Buatlah model <i>Ensemble</i> dengan menggabungkan model #1, #2, dan #3. Anda dapat menggunakan metode <i>Majority Vote</i> (class <i>Majority Voter</i> sudah kami sediakan) atau metode <i>Stacking</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority Voter Class\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "class MajorityVoteClassifier(BaseEstimator, \n",
    "                             ClassifierMixin):\n",
    "    \"\"\" A majority vote ensemble classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifiers : array-like, shape = [n_classifiers]\n",
    "      Different classifiers for the ensemble\n",
    "    vote : str, {'classlabel', 'probability'} (default='label')\n",
    "      If 'classlabel' the prediction is based on the argmax of\n",
    "        class labels. Else if 'probability', the argmax of\n",
    "        the sum of probabilities is used to predict the class label\n",
    "        (recommended for calibrated classifiers).\n",
    "    weights : array-like, shape = [n_classifiers], optional (default=None)\n",
    "      If a list of `int` or `float` values are provided, the classifiers\n",
    "      are weighted by importance; Uses uniform weights if `weights=None`.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value\n",
    "                                  in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Matrix of training samples.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Vector of target class labels.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability' or 'classlabel'\"\n",
    "                             \"; got (vote=%r)\"\n",
    "                             % self.vote)\n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d classifiers'\n",
    "                             % (len(self.weights), len(self.classifiers)))\n",
    "        # Use LabelEncoder to ensure class labels start with 0, which\n",
    "        # is important for np.argmax call in self.predict\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Matrix of training samples.\n",
    "        Returns\n",
    "        ----------\n",
    "        maj_vote : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "            \n",
    "        \"\"\"\n",
    "        if self.vote == 'probability':\n",
    "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
    "        else:  # 'classlabel' vote\n",
    "            #  Collect results from clf.predict calls\n",
    "            predictions = np.asarray([np.reshape(clf.predict(X), 90)\n",
    "                                      for clf in self.classifiers]).T\n",
    "            maj_vote = np.apply_along_axis(\n",
    "                                      lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions.astype(np.int64))\n",
    "        #maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        avg_proba : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "        \"\"\"\n",
    "        probas = np.asarray([clf.predict_proba(X)\n",
    "                             for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "        return avg_proba\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Get classifier parameter names for GridSearch\"\"\"\n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_classifiers.copy()\n",
    "            for name, step in six.iteritems(self.named_classifiers):\n",
    "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Anda tidak perlu men-training Majority Voter lagi karena model - model yang menjadi \\n    komposisi Majority Voter sudah dilatih sebelumnya\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh pemakaian MajoritVoteClassifier\n",
    "\n",
    "mjv_model = MajorityVoteClassifier(classifiers=[rf_model, ab_model, fcnn_model])\n",
    "\n",
    "\"\"\"\n",
    "    Anda tidak perlu men-training Majority Voter lagi karena model - model yang menjadi \n",
    "    komposisi Majority Voter sudah dilatih sebelumnya\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perbandingan Performa Model (10 point)\n",
    "\n",
    "Sertakan tabel perbandingan performa model (anda dapat menggunakan F1 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 311us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = mjv_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7719298245614034"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
